{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "Author: Matthew Viteri, Yu Mo\n",
    "### Problem 1 - Warm up. Grid Search CV.\n",
    "1. Run this simple example from scikit learn, and understand what each command is doing:\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_\n",
    "digits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.016) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.959 (+/-0.029) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.026) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.025) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.025) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.975 (+/-0.014) for {'C': 1, 'kernel': 'linear'}\n",
      "0.975 (+/-0.014) for {'C': 10, 'kernel': 'linear'}\n",
      "0.975 (+/-0.014) for {'C': 100, 'kernel': 'linear'}\n",
      "0.975 (+/-0.014) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        89\n",
      "           1       0.97      1.00      0.98        90\n",
      "           2       0.99      0.98      0.98        92\n",
      "           3       1.00      0.99      0.99        93\n",
      "           4       1.00      1.00      1.00        76\n",
      "           5       0.99      0.98      0.99       108\n",
      "           6       0.99      1.00      0.99        89\n",
      "           7       0.99      1.00      0.99        78\n",
      "           8       1.00      0.98      0.99        92\n",
      "           9       0.99      0.99      0.99        92\n",
      "\n",
      "    accuracy                           0.99       899\n",
      "   macro avg       0.99      0.99      0.99       899\n",
      "weighted avg       0.99      0.99      0.99       899\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.019) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.957 (+/-0.029) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.981 (+/-0.028) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.981 (+/-0.026) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.981 (+/-0.026) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.972 (+/-0.012) for {'C': 1, 'kernel': 'linear'}\n",
      "0.972 (+/-0.012) for {'C': 10, 'kernel': 'linear'}\n",
      "0.972 (+/-0.012) for {'C': 100, 'kernel': 'linear'}\n",
      "0.972 (+/-0.012) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        89\n",
      "           1       0.97      1.00      0.98        90\n",
      "           2       0.99      0.98      0.98        92\n",
      "           3       1.00      0.99      0.99        93\n",
      "           4       1.00      1.00      1.00        76\n",
      "           5       0.99      0.98      0.99       108\n",
      "           6       0.99      1.00      0.99        89\n",
      "           7       0.99      1.00      0.99        78\n",
      "           8       1.00      0.98      0.99        92\n",
      "           9       0.99      0.99      0.99        92\n",
      "\n",
      "    accuracy                           0.99       899\n",
      "   macro avg       0.99      0.99      0.99       899\n",
      "weighted avg       0.99      0.99      0.99       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the Digits dataset\n",
    "# Digits dataset contain a set of handwriting digits image\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "# The original dataset shape is (1797, 64)\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "# Two sets of parameters for tuning: find the best of gamma and C for rbf and linear kernel separately\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "# Evaluate the performance of chosen parameters by precision and recall value separately\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    # Use Grid Search Cross Validation method to train models with different parameters and find the best one\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    \n",
    "    # Print out the evaluation scores for each testing parameter combination\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    # classification_report: Build a text report showing the main classification metrics\n",
    "    # Text summary of the precision, recall, F1 score for each class.\n",
    "    # macro average: averaging the unweighted mean per label\n",
    "    # weighted average: averaging the support-weighted mean per label\n",
    "    # micro average: averaging the total true positives, false negatives and false positives\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
    "# output model is the same for precision and recall with ties in quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Lasso, Forward Selection and Cross Validation\n",
    "Use the data generation used in the Lecture 7 notebook, where we first introduced Lasso, to generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some sparse data to play with\n",
    "np.random.seed(7)\n",
    "\n",
    "n_samples, n_features = 100, 200\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "k = 5\n",
    "# beta generated with k nonzeros\n",
    "# coef = 10 * np.random.randn(n_features)\n",
    "coef = 10 * np.ones(n_features)\n",
    "inds = np.arange(n_features)\n",
    "np.random.shuffle(inds)\n",
    "coef[inds[k:]] = 0  # sparsify coef\n",
    "y = np.dot(X, coef)\n",
    "\n",
    "# add noise\n",
    "y += 0.01 * np.random.normal((n_samples,))\n",
    "\n",
    "# Split data in train set and test set\n",
    "n_samples = X.shape[0]\n",
    "X_train, y_train = X[:75], y[:75]\n",
    "X_test, y_test = X[75:], y[75:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Manually implement forward selection. Report the order in which you add features.\n",
    "2. Plot test error as a function of the size of the support. Can you use this to recover the true\n",
    "support?\n",
    "3. Use Lasso with a manually implemented Cross validation using the metric of your choice.\n",
    "What is the value of the hyperparameter? (Manually implemented means that you can either\n",
    "do it entirely on your own, or you can use GridSearchCV, but I’m asking you not to use\n",
    "LassoCV, which you will use in the next problem).\n",
    "4. Change the number of folds in your CV and repeat the previous step. How does the optimal\n",
    "value of the hyperparameter change? Try to explain any trends that you find.\n",
    "5. Read about and use LassoCV from sklearn.linear model. How does this compare with what\n",
    "you did in the previous step? If they agree, then explain why they agree, and if they disagree\n",
    "explain why. This will require you to make sure you understand what LassoCV is doing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forward Selection <br>\n",
    "Steps for Forward Selection: <br>\n",
    "Step 1: Start from the model only contains a interception. <br>\n",
    "Step 2: Do (d-i) times (i+1)-dim regressions and keep the column that improved previous solution the most. <br>\n",
    "Step 3: Repeat step 2 starting from i = 0, until the improvement of model is insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use Linear Regression (ordinary lease squares) as our classifier\n",
    "# Since predicted y is continuous value in Regression Model, we use R2 to evaluate the model\n",
    "def ForwardSelection(X, y, k_feature):\n",
    "    #sample_feature = X_train.shape[1]\n",
    "    sample_feature = X.shape[1]\n",
    "    OLS = LinearRegression()\n",
    "    selected_feature = []\n",
    "    r2_comb = 0\n",
    "    # feature_num^th feature selection\n",
    "    for feature_num in range(k_feature):\n",
    "        #model_dim = feature_num + 1\n",
    "        feature_score = np.zeros(sample_feature)\n",
    "        # (i+1)-dim regression, repeat (d-i) times\n",
    "        #print(feature_num)\n",
    "        for cur_feature in range(sample_feature):\n",
    "            if cur_feature not in selected_feature:\n",
    "                #print(selected_feature+[cur_feature])\n",
    "                #cur_X_train = X_train[:, selected_feature+[cur_feature]]\n",
    "                #cur_X_test = X_test[:, selected_feature+[cur_feature]]\n",
    "                #OLS.fit(cur_X_train, y_train)\n",
    "                #y_pred = OLS.predict(cur_X_test)\n",
    "                #feature_score[cur_feature] = r2_score(y_test, y_pred)\n",
    "                OLS.fit(X[:, selected_feature+[cur_feature]], y)\n",
    "                feature_score[cur_feature] = r2_score(y, OLS.predict(X[:, selected_feature+[cur_feature]]))\n",
    "        # Compared performance improvement with the original combination\n",
    "        #print(np.amax(feature_score), np.where(feature_score == np.amax(feature_score))[0][0])\n",
    "        #print(feature_score)\n",
    "        selected_feature.append(np.where(feature_score == np.amax(feature_score))[0][0])\n",
    "        r2_comb = np.amax(feature_score)\n",
    "    return selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The order in which features are added:  [18, 15, 51, 78, 34]\n"
     ]
    }
   ],
   "source": [
    "selected_order = ForwardSelection(X, y, 5)\n",
    "print(\"The order in which features are added: \", selected_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15, 18, 34, 51, 78], dtype=int64),)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The true coefficients\n",
    "np.where(coef != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'avg_score': 0.16607183960589594,\n",
       "  'cv_scores': array([ 0.15048472,  0.26142896,  0.0302594 ,  0.32701343,  0.0611727 ]),\n",
       "  'feature_idx': (18,),\n",
       "  'feature_names': ('18',)},\n",
       " 2: {'avg_score': 0.3612349902048369,\n",
       "  'cv_scores': array([ 0.45107906,  0.42505292,  0.23562284,  0.31721991,  0.37720022]),\n",
       "  'feature_idx': (18, 34),\n",
       "  'feature_names': ('18', '34')},\n",
       " 3: {'avg_score': 0.51641507870054359,\n",
       "  'cv_scores': array([ 0.67547465,  0.52501453,  0.46442593,  0.57716682,  0.33999346]),\n",
       "  'feature_idx': (15, 18, 34),\n",
       "  'feature_names': ('15', '18', '34')},\n",
       " 4: {'avg_score': 0.69766002879583922,\n",
       "  'cv_scores': array([ 0.87308624,  0.75732946,  0.58101248,  0.67543832,  0.60143364]),\n",
       "  'feature_idx': (15, 18, 34, 78),\n",
       "  'feature_names': ('15', '18', '34', '78')},\n",
       " 5: {'avg_score': 1.0,\n",
       "  'cv_scores': array([ 1.,  1.,  1.,  1.,  1.]),\n",
       "  'feature_idx': (15, 18, 34, 51, 78),\n",
       "  'feature_names': ('15', '18', '34', '51', '78')}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Module that implements forward selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "sfs1 = sfs(LinearRegression(), k_features=5, forward=True, scoring='r2')\n",
    "sfs1 = sfs1.fit(X, y)\n",
    "sfs1.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’. Focus on pages 1-19 (up\n",
    "to Part II), the remaining part is more relevant for communication.\n",
    "http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "Summarize what you learned briefly (e.g. half a page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Scraping, Entropy and ICML papers.\n",
    "ICML – the International Conference on Machine Learning – is a top research conference in Machine\n",
    "learning. Scrape all the pdfs of all ICML 2019 papers from http://proceedings.mlr.press/v97/.\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "1\n",
    "2. Let Z be a randomly selected word in a randomly selected ICML paper. Estimate the entropy\n",
    "of Z.\n",
    "3. Synthesize a random paragraph using the marginal distribution over words.\n",
    "4. (Optional) Synthesize a random paragraph using an n-gram model on words. Synthesize\n",
    "a random paragraph using any model you want. Top five synthesized text paragraphs win\n",
    "bonus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Logistic Regression\n",
    "The following is a logistic regression problem using a real data set, made available by the authors of the book “Applied Regression and Muiltilevel Modeling” by Gelman and Hill.<br>\n",
    "Download the data from the book, which you can find here http://www.stat.columbia.edu/\n",
    "~gelman/arm/software/. In particular, we are interested in the arsenic data set. The file\n",
    "wells.dat contains data on 3,020 households in Bangladesh. For each family, the natural arsenic level of each well was measured. In addition, the distance to the nearest safest well was\n",
    "measured. Each family is also described by a feature that relates to their community involvement,\n",
    "and a feature that gives the education level of the head of household. We are interested in building\n",
    "a model that predicts whether the family decided to switch wells or not, based on being informed\n",
    "of the level of arsenic in the well. Thus the “label” for this problem is the binary vector that is the\n",
    "first column of the dataset, labeled “switch.” <br>\n",
    "1. Fit a logistic regression model using only an offset term and the distance to the nearest safe well. \n",
    "2. Plot your answer: that is, plot the probability of switching wells as a function of the distance to the nearest safe well.\n",
    "3. Interpreting logistic regression coefficients: Use the “rule-of-4” discussed in class on Thursday, to interpret the solution: what can you say about the change in the probability of switching wells, for every additional 100 meters of distance?\n",
    "4. Now solve a logistic regression incorporating the constant term, the distance and also arsenic levels. Report the coefficients\n",
    "5. Next we want to answer the question of which factor is more significant, distance, or arsenic levels? This is not a well specified question, since these two features have different units.\n",
    "One natural choice is to ask if after normalizing by the respective standard deviations of each feature, if moving one unit in one (normalized) feature predicts a larger change in probability of switching wells, than moving one unit in the other (also normalized) feature. Use this reasoning to answer the question.\n",
    "6. Now consider all the features in the data set. Also consider adding interaction terms among all features that have a large main effect. Use cross validation to build the best model you can (using your training set only), and then report the test error of your best model.1\n",
    "7. (Optional) Now also play around with $l_1$ and $l_2$ regularization, and try to build the most accurate model you can (accuracy computed on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
